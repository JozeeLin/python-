# 第八章使用机器学习进行情感分析

在本章中，我们将深入研究自然语言处理领域的一个分支——情感分析，还将学习如何使用机器学习算法基于文档的情感倾向。涉及到的主题有:

- 清洗和准备文本数据
- 基于文本文档构建特征向量
- 训练机器学习模型用于区分电影的正面与负面评论
- 使用out-of-core学习处理大规模文本数据集

## 情感分析

**情感分析**，有时也称为**观点挖掘(opinion mining)，**是NLP领域一个非常流行的分支；它分析的是文档的**情感倾向(polarity)**。情感分析的一个常见任务就是**根据作者对某一主题所表达的观点或是情感来对文档进行分类**。

在本章中，我们将使用由maas等人收集的互联网电影数据库中的大量电影评论数据。此数据集包含50000个关于电影的正面或负面的评论，**正面的意思是影片在IMDb数据库中的评分高于6星**，而**负面的意思是影片的评分低于5星**。在本章后续内容中，我们将学习如何从这些电影评论的子集中抽取有意义的信息，以此来构建模型并用于预测评论者对影片的喜好。

## 词袋模型简介

前面的[第四章](https://github.com/JozeeLin/python-machine-learning/blob/master/%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E7%AB%A0%20%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86-%E6%9E%84%E5%BB%BA%E5%A5%BD%E7%9A%84%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE%E9%9B%86.md)中，我们需要将文本或者单词等分类数据转换为数值格式，以方便在机器学习算法中使用。在本节中介绍的**词袋模型(bag-of-words model)**，它将文本以数值特征向量的形式来表示。词袋模型的理念很简单，可描述如下:

1. 在整个文档集上为每个词汇创建了唯一的标记，例如单词
2. 为每个文档构建一个特征向量，其中包含每个单词在此文档中出现的次数

由于每个文档中出现的单词数量只是整个词袋中单词总量很小的一个子集，因此特征向量中的大多数元素为0，这也是我们称之为**稀疏(sparse)**的原因。

### 将单词转换为特征向量

根据每个文档中的单词数量构建词袋模型，可以使用scikit-learn中的Count-Vectorizer类。

**原始词频:**出现在特征向量中的值，表示为tf(t,d)，词汇t在文档d中出现的次数。

**一元组(1-gram)或单元组(unigram)模型:**词汇表中的每一项或每个单元代表一个词汇。

**n元组(n-gram):**词汇表中每一项代表n个词汇。

### 通过词频-逆文档频率计算单词关联度

当我们分析文本数据时，经常遇到的问题就是:一个单词出现在两种类型的多个文档中。这种频繁出现的单词通常不包含有用或具备辨识度的信息。在本小节中，将介绍**词频-逆文档频率(term frequency-inverse document frequency,tf-idf)**的技术。它可以**用于解决特征向量中单词频繁出现的问题**。**tf-idf可以定义为词频与逆文档频率的乘积**:
$$
tf-idf(t,d) = tf(t,d)×idf(t,d)
$$
**逆文档频率idf(t,d)**，通过如下公式计算:
$$
idf(t,d) = \log \frac{n_d}{1+df(d,t)}
$$
$n_d$为文档的总数，$df(d,t)$为包含词汇t的文档d的数量。分母加1，用于保证没有出现在任何训练样本中的词汇情况下，分母不为0。取对数是为了保证文档中出现频率较低的词汇不会被赋予过大的权重。

使用sklearn中的另外一个转换器:TfidfTransformer，它以CountVectorizer的原始词频作为输入，并将其转换为tf-idf。

sklearn中idf和tf-idf的定义为:
$$
idf(t,d) = \log \frac{1+n_d}{1+df(d,t)} \\
tf-idf(t,d) = tf(t,d)×(idf(t,d)+1)
$$
举例说明TfidfTransformer的工作方式，计算第三个文档中单词is的tf-idf：

```
    ['The sun is shining',
    'The weather is sweet',
    'The sun is shining and the weather is sweet']
```

在文档3(第三句话)中，单词is的词频是2(tf=2)，由于is在三个文档中都出现过，因此它的文档频率为3(df=3)。因此，idf的计算方法为:
$$
idf("is",d3) = \log \frac{1+3}{1+3} = 0 \\
tf-idf("is",d3) = 2 × (0+1) = 2
$$
那么如果对文档3所有条目重复此过程，得到的tf-idf向量:[1.69,2.00,1.29,1.29,1.29,2.00,1.29]，最后还需要对tf-idf进行L2归一化。计算方式如下:
$$
\begin{align*}
tf-idf("is",d3)_{norm} &= \frac{[1.69,2.00,1.29,1.29,1.29,2.00,1.29]}{\sqrt{1.69^2+2.00^2+1.29^2+1.29^2+1.29^2+2.00^2+1.29^2}} \\
&= [0.40,0.48,0.31,0.31,0.31,0.48,0.31]
\end{align*}
$$

### 清洗文本数据

> 去除所有不需要的字符对文本数据进行清洗。这里使用正则表达式对不需要的字符进行匹配，然后把它移除。

### 标记文档

把文本词料拆分为单独的元素。

1. 标记文档的一种常用方法就是通过文档的空白字符将其拆分为单独的单词。


2. 另外一种有用的技术为"词干提取"(word stemming)。这里使用nltk中的porter stermming算法进行词干提取。(使单词恢复到原始形式，如running，最终为run)

### 停用词移除(stop-word removal)

停用词指的是在各种文本中太过常见，以致没有(或很少)含有用于区分文本所属类别的有用信息。

### 使用大数据——在线算法与外存学习

在对数据量非常大的数据集进行建模时，计算成本非常高。但是我们可以通过一种称为外存学习的技术来处理超大数据集。比如，随机梯度下降或小批量梯度下降。

## 总结

词袋模型虽然在文本分类领域最为流行，但是它没有考虑句子的结构和语法。一种流行的词袋模型扩展就是**潜狄利克雷分配(Latent Dirichlet Allocation)**，这是一种**考虑句子潜在语义的主题模型**。

**word2vec**是最近提出的一种词袋模型的替代算法。**word2vec算法是基于神经网络的一种无监督算法**，它会自动尝试学习单词之间的关系。word2vec算法是基于神经网络的一种无监督算法，它会自动尝试学习单词之间的关系。word2vec背后的理念就是将词义相近的单词划分到相同的簇中；通过巧妙的向量间隔，此模型可以通过简单的向量计算来得到合适的单词。例如:king-man+woman=queen。[项目地址](https://code.google.com/p/word2vec)(这里可以找到模型原始的C语言实现、相关的论文及其替代实现代码)。

## 数据集链接

