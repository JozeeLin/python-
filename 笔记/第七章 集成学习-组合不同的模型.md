# 集成学习——组合不同的模型

在简单机器学习算法的基础上，探索一种新的方法，构建一组分类器的集合，使得整体分类效果优于其中任意一个单独分类器。本章内容包括：

- 基于多数投票的预测
- 通过对训练数据集的重复抽样和随机组合降低模型的过拟合
- 通过弱学习机在误分类数据上的学习构建性能更好的模型

## 集成学习

> **集成方法**的目标是：将不同的分类器组合成为一个元分类器，与包含于其中的单个分类器相比，元分类器具有更好的泛化性能。

### 多数投票

多数投票原则是指将得票数超过50%的结果作为类标。严格来说，多数投票只适用于二分类情形。把多数投票原则推广到多分类，也叫作简单多数票法。

假设二分类中的n个成员分类器都有相同的出错率$\epsilon$,且出错率之间是相互独立的，二分类的集成学习模型的出错率服从多项式分布：
$$
P(y \geq k) = \sum_{k}^{n} \langle \begin{matrix} n \\ k \end{matrix} \rangle \epsilon^k(1-\epsilon)^{n-k} = \epsilon_{ensemble}
$$
举个例子，假设使用11个分类器，单个分类器的出错率为0.25:
$$
P(y \geq k) = \sum_{k=6}^{n} \langle \begin{matrix} 11 \\ k \end{matrix} \rangle 0.25^k(1-0.25)^{11-k} = 0.034
$$
正如我们所见，在满足所有的假设条件下，集成后的出错率远远小于单个分类器的出错率。

## 多数投票分类器

