# 集成学习——组合不同的模型

在简单机器学习算法的基础上，探索一种新的方法，构建一组分类器的集合，使得整体分类效果优于其中任意一个单独分类器。本章内容包括：

- 基于多数投票的预测
- 通过对训练数据集的重复抽样和随机组合降低模型的过拟合
- 通过弱学习机在误分类数据上的学习构建性能更好的模型

## 集成学习

> **集成方法**的目标是：将不同的分类器组合成为一个元分类器，与包含于其中的单个分类器相比，元分类器具有更好的泛化性能。

### 多数投票

多数投票原则是指将得票数超过50%的结果作为类标。严格来说，多数投票只适用于二分类情形。把多数投票原则推广到多分类，也叫作简单多数票法。

假设二分类中的n个成员分类器都有相同的出错率$\epsilon$,且出错率之间是相互独立的，二分类的集成学习模型的出错率服从多项式分布：
$$
P(y \geq k) = \sum_{k}^{n} \langle \begin{matrix} n \\ k \end{matrix} \rangle \epsilon^k(1-\epsilon)^{n-k} = \epsilon_{ensemble}
$$
举个例子，假设使用11个分类器，单个分类器的出错率为0.25:
$$
P(y \geq k) = \sum_{k=6}^{n} \langle \begin{matrix} 11 \\ k \end{matrix} \rangle 0.25^k(1-0.25)^{11-k} = 0.034
$$
正如我们所见，在满足所有的假设条件下，集成后的出错率远远小于单个分类器的出错率。

## 多数投票分类器

多数投票法也称为堆叠(stacking)。不过，堆叠算法更典型地应用于组合逻辑斯蒂回归模型，以各独立分类器的输出作为输入，通过对这些输入结果的继承来预测最终的类标。

## bagging——通过bootstrap样本构建集成分类器

bagging没有使用相同的训练集拟合集成分类器中的单个成员分类器。

### bootstrap抽样

有放回的随机抽样。如下图所示，我们有7个不同的训练样例(使用索引1-7来表示)，在每一轮的bagging循环中，它们都被可放回随机抽样。

## boosting

提高弱学习机的性能。自适应Boosting——adaboost。

**弱分类器**：分类性能稍好于随机猜测的分类器。典型的弱学习机例子就是单层决策树。

Boosting主要针对难以区分的训练样本，也就是说，弱学习机通过在错误分类样本熵的学习来提高集成分类的性能。**与bagging不同，在boosting的初始化阶段，算法使用无放回抽样从训练样本中随机抽取一个子集**。原始的boosting过程可总结为如下四步骤：

1. 从训练集D中以无放回抽样的方式随机抽取一个训练子集$d_1$,用于弱学习机$C_1$的训练
2. 从训练集中以无放回抽样方式随机抽取第2个训练子集$d_2$,并将$C_1$中误分类样本的50%加入到训练集中，训练得到弱学习机$C_2$。
3. 从训练集D中抽取$C_1$和$C_2$分类结果不一致的样本生成训练样本集$d_3$,以此训练第3个弱学习机$C_3$
4. 通过多数投票组合三个弱学习机$C_1$,$C_2$和$C_3$

### Adaboosting

使用整个训练集来训练弱学习机，其中训练样本在每次迭代中都会重新被赋予一个权重，在上一弱学习机错误的基础上进行学习进而构建一个更加强大的分类器。

以二分类为例，×表示向量的元素相乘，点号(.)表示两个向量的内积。算法步骤如下:

1. 以等值方式为权重向量w赋值，其中$\sum_{i} w_i = 1$
2. 在m轮boosting操作中，对第j轮做如下操作:
3. 训练一个加权的弱学习机，$C_j = train(X,y,w)$
4. 预测样本类标$\hat{y} = predict(C_j, X)$
5. 计算权重错误率$\epsilon=W.(\hat{y}==y)$
6. 计算相关系数:$a_j = 0.5\log \frac{1-\epsilon}{\epsilon}$
7. 更新权重: