# 第三章 使用sklearn实现机器学习算法

## 分类算法的选择

训练机器学习算法五步骤:

1. 特征选择
2. 确定性能评价标准
3. 选择模型及其优化算法
4. 对模型性能进行评估
5. 算法调优

## 分类算法

### 感知器

- 优点
  - 算法简单易于理解和实现
- 缺点
  - 在样本不是完全线性可分的情况下，算法永远不会收敛

### 逻辑斯谛回归

### 支持向量机

#### 核技巧

gamma参数表示高斯球面的截至参数(cut-off parameter)。减小$\gamma$的值，将会增加受影响的训练样本的范围，将导致决策边界更加宽松，该值过小就会导致欠拟合问题。如果$\gamma$过大，则会导致过拟合的情况。

### 决策树

#### 最大化信息增益

### 随机森林

> 通过随机森林将弱分类器集成为强分类器。集成学习的基本理念就是将弱分类器集成为鲁棒性更强的模型，及一个能力更强的分类器，集成后具备更好的泛化误差，不易产生过拟合现象。

随机森林算法可以概括为四个简单的步骤:

1. 使用boostrap抽样方法随机选择n个样本用于训练(从训练集中随机可重复地选择n个样本)
2. 使用步骤1选定的样本构造一颗决策树。节点划分规则为:
   1. 不重复的随机选择d个特征
   2. 根据目标函数的要求，使用选定的特征对节点进行划分
3. 重复上述过程1-2000次
4. 汇总每颗决策树的类标进行多数投票。

随机森林的超参数为:

- boostrap抽样的数量n。通过n可以控制随机森林的偏差与方差的权衡，n值越大降低了随机性，由此更可能导致随机森林的过拟合。反之n过小会导致随机森林的欠拟合。$n=m$ ，m为训练集的样本数。
- 节点划分中使用的特征数量$d=\sqrt{m}$，m为训练集中特征的总量。

**优点**

1. 不必担心超参数的选择
2. 不需要对随机森林进行剪枝
3. 对噪声的鲁棒性更好

### k近邻算法

knn算法学习步骤：

1. 选择近邻的数量k和距离度量方法
2. 找到待分类样本的k个最近邻居
3. 根据最近邻的类标进行多数投票

**缺点**

- 容易出现过拟合问题
  ​

## 适用大数据集的分类器

通过使用sklearn提供随机梯度分类器即SGDClassifier来重新实现前面三种分类器，使它们能够应对当数据集很大的时候，无法一次性读入内存中。该类还提供了partial_fit方法支持在线学习。

## 总结

前两种模型在sklearn中的实现都使用了[LIBLINEAR库](http://www.csie.ntu.edu.tw/~cjlin/liblinear/ )。而SVC模型使用了[LIBSVM库](https://www.csie.ntu.edu.tw/~cjlin/libsvm/)，它是一个专门用于SVM的C/C++库。

