# 第十一章聚类分析—处理无类标数据

在前面的章节中，使用监督学习技术来构建机器学习模型，其中训练数据都是事先已经预测结果的，即训练数据中已提供了数据的类标。在本章中，我们介绍聚类分析，它是一种**无监督学习技术**，可以在事先不知道正确结果(即无类标信息或预期输出值)的情况下，发现数据本身所蕴含的结构等信息。聚类的目标是发现数据中自然形成的分组，使得每个簇内样本的相似性大于与其他簇内样本的相似性。

本章涉及一下内容:

- 使用k-means算法发现簇中心
- 使用自底向上的方法构建层次聚类树
- 基于密度聚类方法发现任意形状簇

## 使用k-means算法对相似对象进行分组

聚类(或称为聚类分析)是一种可以找到相似对象群组的技术，与组间对象相比，组内对象之间具有更高的相似度。

k-means算法是**基于原型的聚类**，易于实现，且具有很高的计算效率。基于原型的聚类意味着每个簇都对应一个原型，它可以是一些具有连续型特征的**相似点的中心点(centroid)(平均值)**，或者是类别特征情况下**相似点的众数(medoid)**——最典型或是出现频率最高的点。虽然k-means算法可以高效识别球型簇，但是此算法的**缺点在于必须事先指定先验的簇数量k**。**如果k值选择不当，则可能导致聚类效果不佳**。

后面我们可以通过**肘(elbow)方法**和**轮廓图(silhouette plot)方法**，它们可以**用来评估聚类效果，并帮助我们选出最优k值**。

K-means算法的四个步骤:

1. 从样本点中随机选择k个点作为初始簇中心
2. 将每个样本点划分到距离它最近的中心店$\mu^i$，$j \in {1,\dots,k}$所代表的簇中。
3. 用各簇中所有样本的中心点替代原有的中心点。
4. 重复步骤2和3，直到中心店不变或者达到预定迭代次数时，算法终止

**面临的问题**:如何度量对象之间的相似性?我们可以将相似性定义为距离的倒数，在m维空间中，对于特征取值为连续型实数的聚类分析来说，常用的距离度量标准是欧几里得距离的平方:
$$
d(x,y)^2 = \sum_{j=1}^{m}(x_j-y_j)^2=\|x-y\|_2^2
$$
基于欧几里得度量标准，我们可以将k-means算法描述为一个简单的优化问题:通过迭代使得簇内误差平方和(SSE)最小，也称作簇惯性。
$$
SSE = \sum_{i=1}^{n}\sum_{j=1}^{k}w^{(i,j)}=\|x^i-\mu^i\|_2^2
$$
其中，$\mu^j$为簇j的中心点，如果样本$x^i$属于簇j，则有$w^{(i,j)}=1$,否则$w^{(i,j)}=0$。

> 模型训练的过程中，**可能会发生无法收敛的情况**，特别是当我们设置了较大的max_iter值时，更有可能产生此类问题。**解决收敛问题的一个方法**就是为tol参数设置一个较大的值，此参数控制对簇内误差平方和的容忍度，此容忍度用于判定算法是否收敛。

###k-means++

k-means算法是使用随机点来作为初始中心点，若初始中心点选择不当，有可能会**导致簇效果不佳或产生收敛速度慢等问题**。解决此问题的**一种方案就是在数据集上多次运行k-means算法，并根据误差平方和(SSE)选择性能最好的模型**。**另一种方案就是使用k-means++算法让初始中心点彼此尽可能远离，相比传统k-means算法，它能够产生更好、更一致的结果**。

k-means++算法的初始化过程如下:

1. 初始化一个空的集合M，用于存储选定的k个中心点
2. 从输入样本中随机选定第一个中心点$\mu^j$，并将其加入到集合M中
3. 对于集合M之外的任一样本点$x^i$，通过计算找到与其平台距离最小的样本$d(x^i,M)^2$
4. 使用加权概率分布$\frac{d(\mu^p,M)^2}{\sum_id(x^i,M)^2}$来随机选择下一个中心点$\mu^p$
5. 重复步骤2,3,直到选定k个中心点
6. 基于选定的中心点执行k-means算法

**k-means算法还有另一个问题，就是一个或多个簇的结果为空**。但是**k-medoids**或者**模糊C-means**算法中不存在这种问题。

> 空簇问题在sklearn中存在，如果某个簇为空，算法将搜索距离空簇中心点最远的样本，最后将此最远样本点作为中心点。



**注意:当使用欧几里得距离作为度量标准将k-means算法应用到真实的数据中时，需要进行特征缩放。**

k-means的缺点之一就是指定簇数量k。尤其当面对的是一个无法可视化展现的高维数据集时，这个缺点更加明显。k-means另一个特点就是**簇不可重叠**，也**不可分层**，并且**假定每个簇至少会有一个样本**。

### 硬聚类和软聚类

硬聚类(hard clustering) 指的是数据集中每个样本只能划至一个簇的算法。例如k-means算法。相反，软聚类(soft clustering),有时也称作模糊聚类算法(fuzzy clustering)可以将一个样本划分到一个或者多个簇。一个常见的软聚类算法是模糊C-means(fuzzy C-means, FCM)算法(也称为soft k-means或者fuzzy k-means)。FCM的处理过程与k-means十分相似，但是，我们使用每个样本点隶属于各簇的概率来替代硬聚类的划分。

在k-means中，使用二进制稀疏向量来表示各簇所含样本:
$$
\begin{bmatrix}
\mu^1 \to 0 \\
\mu^2 \to 1 \\
\mu^3 \to 0
\end{bmatrix}
$$
其中，位置索引的值为1表示样本属于簇中心$\mu^j$所在的簇(假定k=3，则$j \in \{1,2,3\}$)。

相反，FMC中的成员隶属向量可以表示如下:
$$
\begin{bmatrix}
\mu^1 \to 0.1 \\
\mu^2 \to 0.85 \\
\mu^3 \to 0.05
\end{bmatrix}
$$
这里，每个值都在区间[0,1]内，代表样本属于相应簇中心所在簇的概率。对于给定样本，其隶属各簇的概率之和为1.与k-means算法类似，FCM算法总结为四个核心步骤:

1. 指定k个中心点，并随机将每个样本点划分至某个簇
2. 计算各簇中心$\mu^j$,$j \in \{1,\dots,k\}$
3. 更新各样本点所属簇的成员隶属度
4. 重复步骤2,3，直到各样本点所属簇成员隶属度不变，或是达到用户自定义的容差阈值或最大迭代次数。

FCM的目标函数简写为$j_m$，形式如下:
$$
J_m = \sum_{i=1}^{n}\sum_{j=1}^{k} W^{m(i,j)}\|x^i-\mu^j\|_2^2 \ , m\in [1,\infty] 
$$
上式中m为模糊系数，用于控制模糊的程度。通常取值为2。
$$
w^{(i,j)} = \left[ \sum_{p=1}^{k}\left( \frac{\|x^i-\mu^j\|_2}{\|x^i-\mu^p\|_2} \right)^{\frac{2}{m-1}} \right]^{-1}
$$
簇中心$\mu^j$可以通过所有样本的加权均值计算得到:
$$
\mu^j = \frac{\sum_{i=1}^{n}w^{m(i,j)}x^i}{\sum_{i=1}^{n}w^{m(i,j)}}
$$
仅就计算样本数据簇的隶属度公式来说，直观上看，FCM的单次迭代计算成本比k-means的要高，但是FCM通常只需要较少的迭代次数便能收敛。

### 使用肘方法确定簇的最佳数量

无监督学习中存在一个问题，就是我们并不知道问题的确切答案。由于没有数据集样本类标的确切数据，所以我们无法在无监督学习算法中使用监督学习算法的评估方法对其进行评估。

为了对聚类效果进行定量分析，我们需要使用模型内部的固有度量来比较不同k-means聚类结果的性能。例如本章先前讨论过的簇内误差平方和(即聚类偏差)。

基于簇内误差平方和，我们可使用图形工具，即所谓的肘方法，针对给定任务估计出最优的簇数量k。直观地看，增加k的值可以降低聚类偏差。**肘方法的基本理念就是找出聚类偏差骤增时的k值**。

### 通过轮廓图定量分析聚类质量

另一种评估聚类质量的定量分析方法是**轮廓分析(silhouette analysis)**。轮廓分析可以使用一个图形工具来度量簇中样本聚集的秘籍程度。通过如下三个步骤计算数据集中单个样本的**轮廓系数(silhouette coefficient)**。

1. 将某一样本$x^i$与簇内其他点之间的平均距离看作是簇的内聚度$a^i$

2. 将样本$x^i$与其最近簇中所有点之间的平均距离看作是与下一最近簇的分离度$b^i$

3. 将簇分离度与簇内聚度之差除以二者中的较大者得到轮廓系数，如下式所示:
   $$
   s^i = \frac{b^i-a^i}{\max\{b^i,a^i\}}
   $$
   轮廓系数的值介于-1到1之间。从上式可见，若簇内聚度与分离度相等$(b^i=a^i)$，则轮廓系数为0.此外，由于$b^i$衡量样本与其他簇内样本间的差异程度，而$a^i$表示样本与簇内其他样本的相似度，因此，如果$b^i \gg a^i$，我们可以近似得到一个值为1的理想的轮廓系数。

   轮廓系数可通过sklearn 中metric模块下的silhouette_samples计算得到，也可选择使用silhouette_scores。

## 层次聚类

本节学习另一种基于原型的聚类:**层次聚类(hierarchical clustering)**。层次聚类算法的**一个优势在于:它能够使我们绘制出树状图(dendrogram,基于二叉层次聚类的可视化)**，这有助于我们使用有意义的分类法解释聚类结果。**层次聚类的另一优势在于我们无需实现指定簇数量**。

层次聚类有两种主要方法:**凝聚(agglomerative)层次聚类**和**分裂(divisive)层次聚类**。

在分裂层次聚类中，我们首先把所有样本看作是在同一个簇中，然后迭代地将簇划分为更小的簇，直到每个簇只包含一个样本。

本节主要介绍凝聚层次聚类，它与分裂层次聚类相反，最初我们把每个样本都看作是一个单独的簇，重复地将最近一对簇进行合并，直到所有的样本都在一个簇为止。

在凝聚层次聚类中，判定簇间距离的两个标准方法分别是**单连接(single linkage)**和**全连接(complete linkage)**。

使用**单连接方法**计算每一对簇中最相似两个样本的距离，并合并距离最近的两个样本所属簇。与之相反，全连接方法是通过比较找到分布于两个簇中最不相似的样本(距离最远的样本)，进而完成簇的合并。

> 凝聚层次聚类中其他常用算法还有**平均连接**和**ward连接**。使用平均连接时，合并两个簇间所有成员间平均距离最小的两个簇。当使用ward连接时，被合并的是使用SSE增量最小的两个簇。

基于全连接方法的凝聚层次聚类，其迭代过程可总结如下:

1. 计算得到所有样本间的距离矩阵
2. 将每个数据点看作是一个单独的簇
3. 基于最不相似(距离最远)样本的距离，合并两个最接近的簇
4. 更新相似矩阵(样本间距离矩阵)
5. 重复步骤2和4，直到所有样本都合并到一个簇为止。

### 基于距离矩阵进行层次聚类

使用scipy中spatial.distanct子模块下的pdist函数来计算距离矩阵，此矩阵作为层次聚类算法的输入。

### 树状图与热度图的关联

在实际应用中，层次聚类的树状图通常与热度图(heat map)结合使用，这样我们可以使用不同的颜色来代表样本矩阵中的独立值。

本节将讨论如何将树状图附加到热度图上，并同时显示在一行上。树状图和热力图的结合步骤:

1. 创建一个figure对象，并通过add_axes属性来设定x轴位置、y轴位置，以及树状图的宽度和高度。
2. 接下来，我们根据树状图对象中的簇类标重排初始化数据框(DataFrame)对象中的数据
3. 基于重排后的数据框(DataFrame)数据，在树状图的右侧绘制热度图
4. 最后，为了美化效果，我们删除了坐标轴标记，并将坐标轴的刻度隐藏。

### 通过sklearn进行凝聚聚类

使用sklearn进行基于凝聚的层次聚类。sklearn中已经实现了一个AgglomerativeClustering类，它允许我们选择待返回簇的数量。当我们想要对层次聚类树进行剪枝时，这个功能是非常有用的。

### 使用DBSCAN划分高密度区域

另外一种聚类算法:(包含噪声情况下)基于密度空间的聚类算法(Density-based Spatial Clustering of Applications with Noise,DBSCAN)。在DBSCAN中，密度被定义为指定半径$\epsilon$范围内样本点的数量。

在DBSCAN中，基于一下标准，每个样本点都被赋予一个特殊的标签:

- 如果在一个点周边的指定半径$\epsilon$内，其他样本点的数量不小于指定数量(MinPts)，则此样本点称为**核心点**(core point)
- 在指定半径$\epsilon$内，如果一个点的邻居点少于MinPts个，但是却包含一个核心点，则此点称为边界点(border point)
- 除了核心点和边界点外的其他样本点称为噪声点(noise point)

完成对核心点、边界点和噪声点的标记后，DBSCAN算法可总结为两个简单的步骤:

1. 基于每个核心点或者一组相邻的核心点(如果核心点的距离很近，则将其看作是相连的)形成了一个单独的簇
2. 将每个边界点划分到其对应核心点所在的簇中。

与k-means不同，DBSCAN的簇空间不一定是球状的，这也是此算法的优势之一。此外，不同于k-means和层次聚类，由于DBSCAN可以识别并移除噪声点，因此它不一定会将所有的样本点都划分到某一簇中。



DBSCAN算法的缺点。对于一个给定样本数量的训练数据集，随着数据集中特征数量的增加，**维度灾难(curse of dimensionality)**的负面影响会随之递增。**在使用欧几里得距离度量时，此问题尤为严重**。不过，使用欧几里得距离度量的聚类算法都面临此问题。

此外，为了能够生成更优的聚类结果，需要对DBSCAN中的两个超参(MinPts,$\epsilon$)调优。如果数据集中的密度差异相对较大，则找到合适的MintPts及$\epsilon$的组合较为困难。

## 延伸

除了上面提到的基于原型的k-means聚类、凝聚层次聚类、使用DBSCAN基于密度的聚类。还有一种更为先进的聚类方法:**图聚类**。图聚类系列中最为突出的方法应该是**谱聚类算法**。虽然实现谱聚类有多种不同的方法，但它们的共同之处在于:均使用基于相似矩阵的特征向量来获得簇间的关系。

更多谱聚类算法请参阅[《A Tutorial on Spectral Clusterin》](https://arxiv.org/pdf/0711.0189v1.pdf)。

