# 第10章 使用回归分析预测连续型目标变量

本章将进入监督学习的另一个分支:回归分析。回归模型可用于连续型目标变量的预测分析，这使得它在探寻变量间关系、评估趋势、做出预测等科学及工业领域应用中极具吸引力。例如，预测公司在未来几个月的销售情况等。

在本章，我们将介绍回归模型的主要概念，将涵盖如下主题:

- 数据集的探索和可视化
- 实现线性回归模型的不同方法
- 训练可处理异常值的回归模型
- 回归模型的评估及常见问题
- 基于非线性数据拟合回归模型

## 简单线性回归模型初探

简单线性回归的目标是:通过模型来描述某一特征(解释变量x)与连续输出(目标变量y)之间的关系。当只有一个解释变量时，线性模型的函数定义如下:
$$
y = w_0 + w_1x
$$
其中，权值$w_0$为函数在y轴上的截距，$w_1$为解释变量的系数。我们的目标是通过学习得到线性方程的这两个权值，并用它们描述解释变量与目标变量之间的关系，当解释变量为非训练数据集中数据时，可用此线性关系来预测对应的输出。

基于前面所定义的线性方程，线性回归可看作是求解样本点的最佳拟合直线。这条最佳拟合线也被称为**回归线**。回归线与样本点之间的垂直连线即所谓的**偏移(offset)或残差(residual)**——预测的误差。

一元线性回归也称为简单线性回归。

多元线性回归的表达式为:
$$
y = w_0x_0+w_1x_1+w_2x_2+\dots+w_mx_m=\sum_{i=0}^{n}w_ix_i = W^TX
$$
其中，$w_0$为$x_0=1$时在y轴上的截距。

## 波士顿房屋数据集

数据集下载:https://archive.ics.uci.edu/ml/datasets/Housing

## 探索性数据分析(Exploratory Data Analysis,EDA)

> 注意:不同于人们通常的理解，训练一个线性回归模型并不需要解释数量或者目标变量呈高斯分布。正态假设仅适用于某些统计检验和假设检验。

使用相关系数矩阵来量化特征之间的相关性。我们可以把相关系数矩阵看作协方差矩阵的标准化版本。实际上，相关系数矩阵就是在将数据标准化后得到的协方差矩阵。

相关系数矩阵是一个包含皮尔逊积矩相关系数(Pearson product-monment correlation coefficient,通常记为Pearson's r)的方阵，它用来衡量两两特征间的线性依赖关系。
$$
r = \frac{\sum_{i=1}^{n}[(x^i-\mu_x)(y^i-\mu_y)]}{\sqrt{\sum_{i=1}^{n}(x^i-\mu_x)^2}\sqrt{\sum_{i=1}^{n}(y^i-\mu_y)^2}} = \frac{\sigma_{xy}}{\sigma_x\sigma_y}
$$
其中，$\mu$为样本对应特征的均值，$\sigma_{xy}$为特征x和y间的协方差，而$\sigma_x$和$\sigma_y$分别为两个特征的标准差。

证明:经标准化各特征间的协方差实际上等价于它们的线性相关系数。

**证明:**
$$
x^{'} = \frac{x-\mu_x}{\sigma_x},y^{'}=\frac{y-\mu_y}{\sigma_y}
$$
两个特征间的协方差:
$$
\sigma_{xy} = \frac{1}{n}\sum_{i}^{n}[(x^i-\mu_x)(y^i-\mu_y)]
$$
由于对特征进行标准化之后，均值都为0，所以最终可得到:
$$
\sigma^{'}_{xy} = \frac{\sigma_{xy}}{\sigma_x\sigma_y}
$$
我们可以通过numpy的corrcoef函数计算前面散点图矩阵中5个特征间的相关系数矩阵。并使用seaborn的heatmap函数绘制其对应的热度图。

## 基于最小二乘法构建线性回归模型

这里我们通过最小二乘法(Ordinary Least Squares,OLS)估计回归曲线的参数，使得回归曲线到样本点垂直距离(残差或误差)的平方和最小。

### 通过梯度下降计算回归参数

## 使用RANSAC拟合高鲁棒性回归模型

异常值对线性回归模型具有严重的影响。某些情况下，数据集的一个非常小的子集也可能会对模型参数的估计造成很大的影响。

随机抽样一致性算法(RANSAC),可用于清除异常值。

RANSAC算法的工作流程如下:

1. 从数据集中随机抽取样本构建内点集合来拟合模型
2. 使用剩余数据对上一步得到的模型进行测试，并**将落在预定公差范围内的样本点增至内点集合**中
3. 使用全部的内点集合数据再次进行模型的拟合
4. 使用内点集合来估计模型的误差
5. 如果模型性能达到了用户设定的特定阈值或者迭代达到了预定的次数，则算法终止，否则跳转到第一步。

## 线性回归模型性能的评估

### 残差图

残差是真实值与预测值之间的差异或者垂直距离。从而对回归模型进行评估。残差图作为常用的图形分析方法，可对回归模型进行评估、获取**模型的异常值**，同时还可检查**模型是否是线性**的，以及**误差是否随机分布**。

完美的预测结果其残差为0。但是这种情况永远不会发生。对于一个好的回归模型，我们期望误差是随机分布的，同时残差也随机分布于中心线附近。如果从残差图中找出规律，意味着模型遗漏了某些能够影响残差的解释信息。

### 均方误差(Mean Squared Error,MSE)

均方误差是线性回归模型拟合过程中，最小化误差平方和(SSE)代价函数的平均值。MSE可用于不同回归模型的比较，或是通过网格搜索进行参数调优，以及交叉验证等。
$$
MSE = \frac{1}{n} \sum_{i=1}^{n}(y^i-\hat{y}^i)^2
$$

### 决定系数$R^2$

决定系数为MSE的标准化版本。用于更好的解释模型的性能。换句话说，$R^2$是模型捕获的响应方差的分数。
$$
R^2 = 1-\frac{SSE}{SST}
$$
其中，SSE为误差平方和，而$SST=\frac{1}{n}\sum_{i=1}^{n}(y^i-\mu_y)^2$,换句话说，$R^2$就是预测值的方差。

使用MSE定义的$R^2$:
$$
\begin{align*}
R^2 &= 1-\frac{SSE}{SST} \\
&= 1-\frac{\frac{1}{n}\sum_{i=1}^{n}(y^i-\hat{y^i})^2}{\frac{1}{n}\sum_{i=1}^{n}(y^i-\mu_y)^2} \\
&= 1-\frac{MSE}{Var(y)}
\end{align*}
$$
对于训练数据集来说，$R^2$的取值范围介于区间[0,1],对于测试集来说，其值可能为负。如果$R^2=1$,此时MSE=0，意味着模型完美拟合了数据。

## 回归中的正则化方法

正则化是通过在模型中加入额外信息来解决过拟合问题的一种方法，引入罚项增加了模型的复杂性但却降低了模型参数的影响。最常见的正则化线性回归方法就是所谓的岭回归(Ridge Regression)、最小绝对收缩及算子选择(Least Absolute Shrinkage and Selection Operator,LASSO)以及弹性网络(Elastic Net)等。

岭回归是基于L2罚项的模型:
$$
J(W)_{Ridge} = \sum_{i=1}^{n}(y^i-\hat{y^i})^2+\lambda\|W\|_2^2
$$
其中，
$$
L2: \lambda\|W\|_2^2 = \lambda \sum_{j=1}^{m}w_j^2
$$
通过增加超参$\lambda$的值，可以增加正则化的强度，同时也就降低了权重对模型的影响。请注意，正则化项不影响截距项$w_0$。

对于基于稀疏数据训练的模型，还有另外啊一种解决方案，即LASSO。基于正则化项的强度，某些权重可以变为0，这也使得LASSO成为一种**监督特征选择技术**。
$$
J(W)_{LASSO} = \sum_{i=1}^{n}(y^i-\hat{y^i})^2+\lambda\|W\|_1
$$
其中，
$$
L2: \lambda\|W\|_1 = \lambda \sum_{j=1}^{m}\|w_j\|
$$
不过LASSO存在一个限制，即如果m>n，则至多可以完成n个变量的筛选。弹性网络则是岭回归和LASSO之间的一个折中。其中包含一个用于稀疏化的L1罚项，以及一个消除LASSO限制的L2罚项。
$$
J(W)_{ElasticNet} = \sum_{i=1}^{n}(y^i-\hat{y^i})^2+\lambda_1\|W\|_2^2 +\lambda_2\|W\|_1
$$

## 线性回归模型的曲线化-多项式回归

对于不符合线性假设的问题，一种常用的解释方法就是通过加入多项式项来使用多项式回归模型。虽然我们可以使用多项式回归对非线性关系建模，但由于线性回归系数w的缘故，多项式回归仍旧被看作是多元线性回归模型。

## 使用随机森林处理非线性关系

### 决策树回归

当使用决策树进行分类时，定义熵作为不纯度的衡量标准，旨在通过最大信息增益对特征进行划分，由此定义如下二分规则:
$$
IG(D_p,x) = I(D_p)-\frac{1}{N}I
$$
其中，x为待划分特征，$N_p$为父节点中样本数量，I为不纯度函数，$D_p$为父节点中训练样本的子集。使用熵作为不纯度度量，这时用于分类的一个有效标准。**为了将决策树用于回归，我们使用MSE替代熵作为节点t的不纯度度量标准**:
$$
I(t) = MSE(t) = \frac{1}{N_t}\sum_{i\in D_t}(y^i-\hat{y}_t)^2
$$
其中，$N_t$为节点t中训练样本的数量，$D_t$为节点t中训练样本的子集，$y^i$为真实的目标值，$\hat{y}_t$为预测目标值(样本均值):
$$
\hat{y}_t = \frac{1}{N}\sum_{i\in D_t}y^i
$$
MSE用于决策树回归时，也常称为**节点内方差**，这就是分裂标准常称为**方差缩减(variance reduction)**的原因。

### 随机森林回归

随机森林回归算法是组合多颗决策树的一种集成技术。由于**随机性有助于降低模型的方差**，与单棵决策树相比，随机森林通常具有更好的泛化能力。随机森林另一个优势在于：它对数据集中的异常值不敏感，且无需过多的参数调优。